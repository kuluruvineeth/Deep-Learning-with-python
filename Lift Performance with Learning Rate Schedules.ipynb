{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "plain-palestinian",
   "metadata": {},
   "source": [
    "# Learning Rate Schedule For Training Models\n",
    "* The classical algorithm to train neural networks is called **stochastic gradient descent**.\n",
    "* Adapting the learning rate for our stochastic gradient descent optimization procedure can increase performance and reduce training time.(**learning rate annealing** or **adaptive learning rates**).\n",
    "* Two popular and easy to use learning rate schedules are:\n",
    "    * Decrease the learning rate gradually based on the epoch\n",
    "    * Decrease the learning rate using punctuated large drops at specific epochs.\n",
    "\n",
    "* The dataset describes radar returns where the target was free electrons in the ionosphere. It is a binary classification problem where positive cases (g for good) show evidence of some type of structure in the ionosphere and negative cases (b for bad) do not.\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "pressed-scenario",
   "metadata": {},
   "source": [
    "# 1. Time-Based Learning Rate Schedule\n",
    "* The stochastic gradient descent optimization algorithm implementation in the SGD class has an argument called **decay**.\n",
    "* This argument is used in the time-based learning rate decay schedule as follows :\n",
    "\n",
    "LearningRate = LearningRate * 1/1 + decay*epoch\n",
    "\n",
    "* When the decay argument is specified,it will decrease the learning rate from the previous epoch by the given fixed amount.\n",
    "* For example, if we use the initial learning rate value of 0.1 and the decay of 0.001,the first 5 epochs will adapt the learning rate as follows:\n",
    "\n",
    "Epoch Learning Rate\n",
    "1     0.1\n",
    "2     0.0999000999\n",
    "3     0.0997006985\n",
    "4     0.09940249103\n",
    "5     0.09900646517\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "western-sphere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 - 1s - loss: 0.6827 - accuracy: 0.5362 - val_loss: 0.4217 - val_accuracy: 0.8793\n",
      "Epoch 2/50\n",
      "9/9 - 0s - loss: 0.4970 - accuracy: 0.8085 - val_loss: 0.4160 - val_accuracy: 0.9138\n",
      "Epoch 3/50\n",
      "9/9 - 0s - loss: 0.3875 - accuracy: 0.8468 - val_loss: 0.3899 - val_accuracy: 0.9483\n",
      "Epoch 4/50\n",
      "9/9 - 0s - loss: 0.3029 - accuracy: 0.8936 - val_loss: 0.2499 - val_accuracy: 0.9569\n",
      "Epoch 5/50\n",
      "9/9 - 0s - loss: 0.2416 - accuracy: 0.9277 - val_loss: 0.2108 - val_accuracy: 0.9655\n",
      "Epoch 6/50\n",
      "9/9 - 0s - loss: 0.2046 - accuracy: 0.9319 - val_loss: 0.1258 - val_accuracy: 0.9741\n",
      "Epoch 7/50\n",
      "9/9 - 0s - loss: 0.1898 - accuracy: 0.9574 - val_loss: 0.1720 - val_accuracy: 0.9741\n",
      "Epoch 8/50\n",
      "9/9 - 0s - loss: 0.1758 - accuracy: 0.9404 - val_loss: 0.1974 - val_accuracy: 0.9828\n",
      "Epoch 9/50\n",
      "9/9 - 0s - loss: 0.1538 - accuracy: 0.9617 - val_loss: 0.1162 - val_accuracy: 0.9914\n",
      "Epoch 10/50\n",
      "9/9 - 0s - loss: 0.1494 - accuracy: 0.9574 - val_loss: 0.1501 - val_accuracy: 0.9741\n",
      "Epoch 11/50\n",
      "9/9 - 0s - loss: 0.1343 - accuracy: 0.9574 - val_loss: 0.2081 - val_accuracy: 0.9224\n",
      "Epoch 12/50\n",
      "9/9 - 0s - loss: 0.1409 - accuracy: 0.9489 - val_loss: 0.0784 - val_accuracy: 0.9914\n",
      "Epoch 13/50\n",
      "9/9 - 0s - loss: 0.1253 - accuracy: 0.9447 - val_loss: 0.0843 - val_accuracy: 0.9914\n",
      "Epoch 14/50\n",
      "9/9 - 0s - loss: 0.1277 - accuracy: 0.9574 - val_loss: 0.1296 - val_accuracy: 0.9914\n",
      "Epoch 15/50\n",
      "9/9 - 0s - loss: 0.1052 - accuracy: 0.9702 - val_loss: 0.0768 - val_accuracy: 0.9914\n",
      "Epoch 16/50\n",
      "9/9 - 0s - loss: 0.1051 - accuracy: 0.9745 - val_loss: 0.1106 - val_accuracy: 0.9914\n",
      "Epoch 17/50\n",
      "9/9 - 0s - loss: 0.1014 - accuracy: 0.9787 - val_loss: 0.1207 - val_accuracy: 0.9828\n",
      "Epoch 18/50\n",
      "9/9 - 0s - loss: 0.0973 - accuracy: 0.9745 - val_loss: 0.1001 - val_accuracy: 0.9914\n",
      "Epoch 19/50\n",
      "9/9 - 0s - loss: 0.0900 - accuracy: 0.9787 - val_loss: 0.0797 - val_accuracy: 0.9914\n",
      "Epoch 20/50\n",
      "9/9 - 0s - loss: 0.0906 - accuracy: 0.9787 - val_loss: 0.1119 - val_accuracy: 0.9914\n",
      "Epoch 21/50\n",
      "9/9 - 0s - loss: 0.0919 - accuracy: 0.9745 - val_loss: 0.0696 - val_accuracy: 0.9914\n",
      "Epoch 22/50\n",
      "9/9 - 0s - loss: 0.0806 - accuracy: 0.9787 - val_loss: 0.0969 - val_accuracy: 0.9914\n",
      "Epoch 23/50\n",
      "9/9 - 0s - loss: 0.0783 - accuracy: 0.9830 - val_loss: 0.0649 - val_accuracy: 0.9914\n",
      "Epoch 24/50\n",
      "9/9 - 0s - loss: 0.0784 - accuracy: 0.9830 - val_loss: 0.0857 - val_accuracy: 0.9914\n",
      "Epoch 25/50\n",
      "9/9 - 0s - loss: 0.0742 - accuracy: 0.9787 - val_loss: 0.0796 - val_accuracy: 0.9914\n",
      "Epoch 26/50\n",
      "9/9 - 0s - loss: 0.0703 - accuracy: 0.9830 - val_loss: 0.0701 - val_accuracy: 0.9914\n",
      "Epoch 27/50\n",
      "9/9 - 0s - loss: 0.0707 - accuracy: 0.9830 - val_loss: 0.0906 - val_accuracy: 0.9914\n",
      "Epoch 28/50\n",
      "9/9 - 0s - loss: 0.0767 - accuracy: 0.9787 - val_loss: 0.0671 - val_accuracy: 0.9914\n",
      "Epoch 29/50\n",
      "9/9 - 0s - loss: 0.0671 - accuracy: 0.9830 - val_loss: 0.0636 - val_accuracy: 0.9914\n",
      "Epoch 30/50\n",
      "9/9 - 0s - loss: 0.0667 - accuracy: 0.9830 - val_loss: 0.0618 - val_accuracy: 0.9914\n",
      "Epoch 31/50\n",
      "9/9 - 0s - loss: 0.0659 - accuracy: 0.9830 - val_loss: 0.0878 - val_accuracy: 0.9914\n",
      "Epoch 32/50\n",
      "9/9 - 0s - loss: 0.0624 - accuracy: 0.9872 - val_loss: 0.0634 - val_accuracy: 0.9914\n",
      "Epoch 33/50\n",
      "9/9 - 0s - loss: 0.0630 - accuracy: 0.9872 - val_loss: 0.0601 - val_accuracy: 0.9914\n",
      "Epoch 34/50\n",
      "9/9 - 0s - loss: 0.0625 - accuracy: 0.9830 - val_loss: 0.0530 - val_accuracy: 0.9914\n",
      "Epoch 35/50\n",
      "9/9 - 0s - loss: 0.0632 - accuracy: 0.9872 - val_loss: 0.0638 - val_accuracy: 0.9914\n",
      "Epoch 36/50\n",
      "9/9 - 0s - loss: 0.0592 - accuracy: 0.9872 - val_loss: 0.0780 - val_accuracy: 0.9914\n",
      "Epoch 37/50\n",
      "9/9 - 0s - loss: 0.0553 - accuracy: 0.9915 - val_loss: 0.0519 - val_accuracy: 0.9914\n",
      "Epoch 38/50\n",
      "9/9 - 0s - loss: 0.0550 - accuracy: 0.9872 - val_loss: 0.0589 - val_accuracy: 0.9914\n",
      "Epoch 39/50\n",
      "9/9 - 0s - loss: 0.0513 - accuracy: 0.9872 - val_loss: 0.0645 - val_accuracy: 0.9914\n",
      "Epoch 40/50\n",
      "9/9 - 0s - loss: 0.0505 - accuracy: 0.9915 - val_loss: 0.0581 - val_accuracy: 0.9914\n",
      "Epoch 41/50\n",
      "9/9 - 0s - loss: 0.0503 - accuracy: 0.9915 - val_loss: 0.0551 - val_accuracy: 0.9914\n",
      "Epoch 42/50\n",
      "9/9 - 0s - loss: 0.0536 - accuracy: 0.9872 - val_loss: 0.0739 - val_accuracy: 0.9914\n",
      "Epoch 43/50\n",
      "9/9 - 0s - loss: 0.0544 - accuracy: 0.9915 - val_loss: 0.0459 - val_accuracy: 0.9914\n",
      "Epoch 44/50\n",
      "9/9 - 0s - loss: 0.0587 - accuracy: 0.9830 - val_loss: 0.0747 - val_accuracy: 0.9914\n",
      "Epoch 45/50\n",
      "9/9 - 0s - loss: 0.0490 - accuracy: 0.9915 - val_loss: 0.0519 - val_accuracy: 0.9914\n",
      "Epoch 46/50\n",
      "9/9 - 0s - loss: 0.0501 - accuracy: 0.9872 - val_loss: 0.0633 - val_accuracy: 0.9914\n",
      "Epoch 47/50\n",
      "9/9 - 0s - loss: 0.0455 - accuracy: 0.9915 - val_loss: 0.0573 - val_accuracy: 0.9914\n",
      "Epoch 48/50\n",
      "9/9 - 0s - loss: 0.0456 - accuracy: 0.9915 - val_loss: 0.0571 - val_accuracy: 0.9914\n",
      "Epoch 49/50\n",
      "9/9 - 0s - loss: 0.0484 - accuracy: 0.9915 - val_loss: 0.0508 - val_accuracy: 0.9914\n",
      "Epoch 50/50\n",
      "9/9 - 0s - loss: 0.0471 - accuracy: 0.9915 - val_loss: 0.0577 - val_accuracy: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb86933f710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Based Learning Rate Decay\n",
    "import pandas\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# fix random seed for reproducibility'\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv('ionosphere.csv',header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split into input(X) and output(Y) variables\n",
    "X = dataset[:,0:34].astype(float)\n",
    "Y = dataset[:,34]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34,input_dim=34,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# compile model\n",
    "epochs = 50\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate/epochs\n",
    "momentum = 0.8\n",
    "sgd = SGD(lr=learning_rate,momentum=momentum,decay=decay_rate,nesterov=False)\n",
    "model.compile(loss='binary_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X,Y,validation_split=0.33,epochs=epochs,batch_size=28,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-proposal",
   "metadata": {},
   "source": [
    "# 2. Drop-Based Learning Rate Schedule\n",
    "* It is to systematically drop the learning rate at specific times during training.\n",
    "* Often this method is implemented by dropping the learning rate by half every fixed number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "laden-catholic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 - 1s - loss: 0.5828 - accuracy: 0.6979 - val_loss: 0.3812 - val_accuracy: 0.9310\n",
      "Epoch 2/50\n",
      "9/9 - 0s - loss: 0.3616 - accuracy: 0.8766 - val_loss: 0.3185 - val_accuracy: 0.9569\n",
      "Epoch 3/50\n",
      "9/9 - 0s - loss: 0.2745 - accuracy: 0.8936 - val_loss: 0.1507 - val_accuracy: 0.9655\n",
      "Epoch 4/50\n",
      "9/9 - 0s - loss: 0.1841 - accuracy: 0.9447 - val_loss: 0.0852 - val_accuracy: 0.9828\n",
      "Epoch 5/50\n",
      "9/9 - 0s - loss: 0.1369 - accuracy: 0.9617 - val_loss: 0.3970 - val_accuracy: 0.7931\n",
      "Epoch 6/50\n",
      "9/9 - 0s - loss: 0.2189 - accuracy: 0.9149 - val_loss: 0.1372 - val_accuracy: 0.9569\n",
      "Epoch 7/50\n",
      "9/9 - 0s - loss: 0.1594 - accuracy: 0.9532 - val_loss: 0.0707 - val_accuracy: 0.9914\n",
      "Epoch 8/50\n",
      "9/9 - 0s - loss: 0.1346 - accuracy: 0.9574 - val_loss: 0.1052 - val_accuracy: 0.9914\n",
      "Epoch 9/50\n",
      "9/9 - 0s - loss: 0.1162 - accuracy: 0.9660 - val_loss: 0.0848 - val_accuracy: 0.9828\n",
      "Epoch 10/50\n",
      "9/9 - 0s - loss: 0.1042 - accuracy: 0.9574 - val_loss: 0.0776 - val_accuracy: 0.9828\n",
      "Epoch 11/50\n",
      "9/9 - 0s - loss: 0.0935 - accuracy: 0.9745 - val_loss: 0.0774 - val_accuracy: 0.9914\n",
      "Epoch 12/50\n",
      "9/9 - 0s - loss: 0.0889 - accuracy: 0.9660 - val_loss: 0.0713 - val_accuracy: 0.9914\n",
      "Epoch 13/50\n",
      "9/9 - 0s - loss: 0.0793 - accuracy: 0.9787 - val_loss: 0.0802 - val_accuracy: 0.9828\n",
      "Epoch 14/50\n",
      "9/9 - 0s - loss: 0.0692 - accuracy: 0.9787 - val_loss: 0.0675 - val_accuracy: 0.9828\n",
      "Epoch 15/50\n",
      "9/9 - 0s - loss: 0.0765 - accuracy: 0.9787 - val_loss: 0.0801 - val_accuracy: 0.9828\n",
      "Epoch 16/50\n",
      "9/9 - 0s - loss: 0.0629 - accuracy: 0.9830 - val_loss: 0.0605 - val_accuracy: 0.9914\n",
      "Epoch 17/50\n",
      "9/9 - 0s - loss: 0.0537 - accuracy: 0.9872 - val_loss: 0.0993 - val_accuracy: 0.9914\n",
      "Epoch 18/50\n",
      "9/9 - 0s - loss: 0.0571 - accuracy: 0.9830 - val_loss: 0.0539 - val_accuracy: 0.9914\n",
      "Epoch 19/50\n",
      "9/9 - 0s - loss: 0.0573 - accuracy: 0.9830 - val_loss: 0.0651 - val_accuracy: 0.9914\n",
      "Epoch 20/50\n",
      "9/9 - 0s - loss: 0.0504 - accuracy: 0.9872 - val_loss: 0.0821 - val_accuracy: 0.9914\n",
      "Epoch 21/50\n",
      "9/9 - 0s - loss: 0.0470 - accuracy: 0.9872 - val_loss: 0.0643 - val_accuracy: 0.9914\n",
      "Epoch 22/50\n",
      "9/9 - 0s - loss: 0.0463 - accuracy: 0.9872 - val_loss: 0.0598 - val_accuracy: 0.9914\n",
      "Epoch 23/50\n",
      "9/9 - 0s - loss: 0.0467 - accuracy: 0.9872 - val_loss: 0.0681 - val_accuracy: 0.9914\n",
      "Epoch 24/50\n",
      "9/9 - 0s - loss: 0.0472 - accuracy: 0.9872 - val_loss: 0.0635 - val_accuracy: 0.9914\n",
      "Epoch 25/50\n",
      "9/9 - 0s - loss: 0.0529 - accuracy: 0.9915 - val_loss: 0.0910 - val_accuracy: 0.9914\n",
      "Epoch 26/50\n",
      "9/9 - 0s - loss: 0.0446 - accuracy: 0.9872 - val_loss: 0.0522 - val_accuracy: 0.9914\n",
      "Epoch 27/50\n",
      "9/9 - 0s - loss: 0.0474 - accuracy: 0.9872 - val_loss: 0.0581 - val_accuracy: 0.9914\n",
      "Epoch 28/50\n",
      "9/9 - 0s - loss: 0.0457 - accuracy: 0.9915 - val_loss: 0.0786 - val_accuracy: 0.9914\n",
      "Epoch 29/50\n",
      "9/9 - 0s - loss: 0.0429 - accuracy: 0.9915 - val_loss: 0.0607 - val_accuracy: 0.9914\n",
      "Epoch 30/50\n",
      "9/9 - 0s - loss: 0.0419 - accuracy: 0.9872 - val_loss: 0.0571 - val_accuracy: 0.9914\n",
      "Epoch 31/50\n",
      "9/9 - 0s - loss: 0.0406 - accuracy: 0.9872 - val_loss: 0.0614 - val_accuracy: 0.9914\n",
      "Epoch 32/50\n",
      "9/9 - 0s - loss: 0.0405 - accuracy: 0.9872 - val_loss: 0.0687 - val_accuracy: 0.9914\n",
      "Epoch 33/50\n",
      "9/9 - 0s - loss: 0.0396 - accuracy: 0.9915 - val_loss: 0.0651 - val_accuracy: 0.9914\n",
      "Epoch 34/50\n",
      "9/9 - 0s - loss: 0.0395 - accuracy: 0.9872 - val_loss: 0.0618 - val_accuracy: 0.9914\n",
      "Epoch 35/50\n",
      "9/9 - 0s - loss: 0.0393 - accuracy: 0.9872 - val_loss: 0.0614 - val_accuracy: 0.9914\n",
      "Epoch 36/50\n",
      "9/9 - 0s - loss: 0.0392 - accuracy: 0.9872 - val_loss: 0.0644 - val_accuracy: 0.9914\n",
      "Epoch 37/50\n",
      "9/9 - 0s - loss: 0.0388 - accuracy: 0.9872 - val_loss: 0.0631 - val_accuracy: 0.9914\n",
      "Epoch 38/50\n",
      "9/9 - 0s - loss: 0.0384 - accuracy: 0.9872 - val_loss: 0.0622 - val_accuracy: 0.9914\n",
      "Epoch 39/50\n",
      "9/9 - 0s - loss: 0.0379 - accuracy: 0.9915 - val_loss: 0.0655 - val_accuracy: 0.9914\n",
      "Epoch 40/50\n",
      "9/9 - 0s - loss: 0.0379 - accuracy: 0.9915 - val_loss: 0.0669 - val_accuracy: 0.9914\n",
      "Epoch 41/50\n",
      "9/9 - 0s - loss: 0.0378 - accuracy: 0.9915 - val_loss: 0.0663 - val_accuracy: 0.9914\n",
      "Epoch 42/50\n",
      "9/9 - 0s - loss: 0.0381 - accuracy: 0.9915 - val_loss: 0.0616 - val_accuracy: 0.9914\n",
      "Epoch 43/50\n",
      "9/9 - 0s - loss: 0.0376 - accuracy: 0.9915 - val_loss: 0.0626 - val_accuracy: 0.9914\n",
      "Epoch 44/50\n",
      "9/9 - 0s - loss: 0.0373 - accuracy: 0.9915 - val_loss: 0.0624 - val_accuracy: 0.9914\n",
      "Epoch 45/50\n",
      "9/9 - 0s - loss: 0.0374 - accuracy: 0.9915 - val_loss: 0.0672 - val_accuracy: 0.9914\n",
      "Epoch 46/50\n",
      "9/9 - 0s - loss: 0.0373 - accuracy: 0.9915 - val_loss: 0.0656 - val_accuracy: 0.9914\n",
      "Epoch 47/50\n",
      "9/9 - 0s - loss: 0.0371 - accuracy: 0.9915 - val_loss: 0.0631 - val_accuracy: 0.9914\n",
      "Epoch 48/50\n",
      "9/9 - 0s - loss: 0.0372 - accuracy: 0.9915 - val_loss: 0.0612 - val_accuracy: 0.9914\n",
      "Epoch 49/50\n",
      "9/9 - 0s - loss: 0.0373 - accuracy: 0.9915 - val_loss: 0.0654 - val_accuracy: 0.9914\n",
      "Epoch 50/50\n",
      "9/9 - 0s - loss: 0.0368 - accuracy: 0.9915 - val_loss: 0.0661 - val_accuracy: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb824771690>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop-Based Learning Rate Decay\n",
    "import pandas\n",
    "import numpy \n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv('ionosphere.csv',header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:34].astype(float)\n",
    "Y = dataset[:,34]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34,input_dim=34,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# compile model\n",
    "sgd = SGD(lr=0.0,momentum=0.9,decay=0.0,nesterov=False)\n",
    "model.compile(loss='binary_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "# learning schedule callback\n",
    "llrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [llrate]\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X,Y,validation_split=0.33,epochs=50,batch_size=28,callbacks=callbacks_list,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-robinson",
   "metadata": {},
   "source": [
    "# Summary\n",
    "* Increase the initial learning rate\n",
    "* Use a large momentum\n",
    "* Experiment with different schedules\n",
    "\n",
    "* Finall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
